# ======================================================================
# üî¨ UNIVERSAL PROTEIN FUNCTION CLASSIFIER (ProtMini v2)
# ======================================================================
# Fetches 5000 reviewed sequences/category from UniProt (human)
# Embeds via pretrained ESM2 (Hugging Face)
# Trains 3 classifiers: MLP, Logistic Regression, Random Forest
# Lets user select which classifier to use for prediction
# Generates a PDF report for selected model
# ======================================================================

!pip install transformers torch biopython reportlab tqdm scikit-learn ipywidgets -q

import os, re, random, torch, requests, numpy as np, pandas as pd
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from torch import nn
from transformers import AutoTokenizer, AutoModel
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from requests.adapters import HTTPAdapter, Retry
from IPython.display import display, clear_output
import ipywidgets as widgets

# ----------------------------------------------------------------------
# STEP 1: FETCH REAL SEQUENCES FROM UNIPROT
# ----------------------------------------------------------------------
categories = {
    "enzyme": "(enzyme) AND (reviewed:true) AND (organism_id:9606)",
    "transport": "(transporter) AND (reviewed:true) AND (organism_id:9606)",
    "signal": "(signal) AND (reviewed:true) AND (organism_id:9606)",
    "structural": "(structural) AND (reviewed:true) AND (organism_id:9606)",
    "immune": "(immune) AND (reviewed:true) AND (organism_id:9606)"
}

def fetch_uniprot(category, query, batch_size=200, limit=5000):
    base = "https://rest.uniprot.org/uniprotkb/search?"
    url = f"{base}query={query.replace(' ', '%20')}&format=fasta&size={batch_size}"
    re_next = re.compile(r'<(.+)>; rel="next"')
    retries = Retry(total=5, backoff_factor=0.3, status_forcelist=[429,500,502,503,504])
    session = requests.Session()
    session.mount("https://", HTTPAdapter(max_retries=retries))

    seqs, seen = [], set()
    while url and len(seqs) < limit:
        r = session.get(url)
        if r.status_code != 200:
            break
        lines = r.text.strip().split("\n")
        for i, line in enumerate(lines):
            if line.startswith(">"):
                if line not in seen and not any(x in line for x in ["UNCHARACTERIZED","PREDICTED","DUMMY"]):
                    seen.add(line)
                    if i+1 < len(lines) and not lines[i+1].startswith(">"):
                        seqs.append(lines[i+1])
        match = re_next.match(r.headers.get("Link", ""))
        url = match.group(1) if match else None

    print(f"‚úÖ {category.capitalize()}: {len(seqs)} sequences")
    return seqs[:limit]

dataset = []
for cat, q in categories.items():
    seqs = fetch_uniprot(cat, q, limit=5000)
    dataset += [(s, cat) for s in seqs]

# Add random ‚Äúother‚Äù category
other = [''.join(random.choices('ACDEFGHIKLMNPQRSTVWY', k=random.randint(80,300))) for _ in range(1000)]
dataset += [(s, "other") for s in other]
random.shuffle(dataset)
print(f"\nüéØ Completed! Total sequences: {len(dataset)}")

# ----------------------------------------------------------------------
# STEP 2: LOAD PRETRAINED ESM2 MODEL
# ----------------------------------------------------------------------
print("\nüì¶ Loading pretrained ESM2 model...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
model = AutoModel.from_pretrained("facebook/esm2_t6_8M_UR50D").to(device)
model.eval()
print("‚úÖ Model ready for embedding.")

# ----------------------------------------------------------------------
# STEP 3: GENERATE EMBEDDINGS
# ----------------------------------------------------------------------
def get_embedding(seq):
    with torch.no_grad():
        inputs = tokenizer(seq, return_tensors='pt', truncation=True, max_length=512).to(device)
        outputs = model(**inputs)
        return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()

print("\nüöÄ Generating embeddings (this may take time)...")
X, y = [], []
le = LabelEncoder()
labels = le.fit_transform([l for _, l in dataset])
for seq, label in tqdm(dataset, desc="Embedding"):
    X.append(get_embedding(seq))
    y.append(label)

X = np.array(X)
y = np.array(labels)

# ----------------------------------------------------------------------
# STEP 4: TRAIN CLASSIFIERS
# ----------------------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ---- MLP Classifier ----
class MLPClassifier(nn.Module):
    def __init__(self, input_dim, n_classes):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, n_classes)
        )
    def forward(self, x):
        return self.fc(x)

mlp = MLPClassifier(X.shape[1], len(le.classes_)).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)

X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)

print("\nüß† Training MLP...")
for epoch in range(5):
    mlp.train()
    optimizer.zero_grad()
    preds = mlp(X_train_tensor)
    loss = criterion(preds, y_train_tensor)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}: loss = {loss.item():.4f}")
mlp.eval()

# ---- Logistic Regression ----
print("\n‚öôÔ∏è Training Logistic Regression...")
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)

# ---- Random Forest ----
print("\nüå≥ Training Random Forest...")
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)

print("\n‚úÖ All classifiers trained successfully!")

# ----------------------------------------------------------------------
# STEP 5: INTERACTIVE PREDICTION + REPORT
# ----------------------------------------------------------------------
seq_box = widgets.Textarea(
    placeholder='Paste your protein sequence here...',
    description='Sequence:',
    layout=widgets.Layout(width='100%', height='150px')
)

model_select = widgets.Dropdown(
    options=['MLP', 'Logistic Regression', 'Random Forest'],
    value='MLP',
    description='Model:',
    layout=widgets.Layout(width='50%')
)

button = widgets.Button(description='üîç Predict Function', button_style='success')
output = widgets.Output()

display(seq_box, model_select, button, output)

def create_pdf_report(sequence, clf_name, pred, conf, sim):
    file = f"protein_prediction_report_{clf_name.replace(' ','_')}.pdf"
    c = canvas.Canvas(file, pagesize=letter)
    c.setFont("Helvetica-Bold", 16)
    c.drawString(50, 750, "Protein Function Prediction Report")
    c.setFont("Helvetica", 12)
    c.drawString(50, 720, f"üß¨ Model: {clf_name}")
    c.drawString(50, 700, f"Sequence (first 60 aa): {sequence[:60]}...")
    c.drawString(50, 680, f"Predicted Class: {pred}")
    c.drawString(50, 660, f"Confidence: {conf*100:.2f}%")
    c.drawString(50, 640, f"2nd Best Class: {sim}")
    c.drawString(50, 620, f"üìÖ Timestamp: {pd.Timestamp.now()}")
    c.save()
    print(f"üìÑ Report saved as {file}")

def on_click(b):
    with output:
        clear_output()
        seq = seq_box.value.strip()
        if not seq:
            print("‚ùå Please paste a valid protein sequence.")
            return
        clf_choice = model_select.value
        emb = get_embedding(seq)

        if clf_choice == 'MLP':
            with torch.no_grad():
                logits = mlp(torch.tensor(emb, dtype=torch.float32).unsqueeze(0).to(device))
                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]
                pred_idx = np.argmax(probs)
                conf = probs[pred_idx]
                sim_idx = np.argsort(probs)[-2]
                pred = le.inverse_transform([pred_idx])[0]
                sim = le.inverse_transform([sim_idx])[0]

        elif clf_choice == 'Logistic Regression':
            probs = lr.predict_proba([emb])[0]
            pred_idx = np.argmax(probs)
            conf = probs[pred_idx]
            sim_idx = np.argsort(probs)[-2]
            pred = le.inverse_transform([pred_idx])[0]
            sim = le.inverse_transform([sim_idx])[0]

        else:  # Random Forest
            probs = rf.predict_proba([emb])[0]
            pred_idx = np.argmax(probs)
            conf = probs[pred_idx]
            sim_idx = np.argsort(probs)[-2]
            pred = le.inverse_transform([pred_idx])[0]
            sim = le.inverse_transform([sim_idx])[0]

        print(f"üß† Model: {clf_choice}")
        print(f"üß© Predicted Function: {pred}")
        print(f"üìä Confidence: {conf*100:.2f}%")
        print(f"üîó Second-best class: {sim}")
        create_pdf_report(seq, clf_choice, pred, conf, sim)

button.on_click(on_click)
